# PyRockDiff - Automatic change-detection workflow for rockfall identification

### üöÄ Quick Overview
This Python-based pipeline automates the comparison of two point clouds obtained from LiDAR or SfM, specifically targeting rock surfaces. It allows for efficient analysis of geological changes with minimal user input.

Key features include:

- **Automation:** The pipeline operates automatically, requiring only a single configuration file for setup.
- **User-Friendly Configuration:** Simplifies the process, making advanced analysis accessible without programming knowledge.
- **Preprocessing:** Cleans point clouds by removing noise and vegetation and aligns them accurately using registration algorithms.
- **Change Detection:** Detects differences between the two analyzed epochs with the M3C2 algorithm.
- **Clustering & Volume Calculation:** Isolates changes using DBSCAN and estimates volumes via alpha-shape triangulation.

## Table of Contents
- [Overview](#overview)
- [Installation and Requirements](#installation-and-requirements)
  - [Installation](#installation)
- [How It Works](#how-it-works)
- [Usage Guide](#usage-guide)
- [Future Updates & Development Stages](#future-updates--development-stages)
- [Contact](#contact)
- [Acknowledgments](#acknowledgments)
- [License](#license)

## Overview

<details>
<summary>Click to expand</summary>

<br>

This project is a Python-based pipeline designed to automate the comparison of two point clouds obtained from LiDAR or Structure from Motion (SfM), specifically focusing on rock surfaces. The pipeline begins with essential pre-processing steps, including data cleaning and vegetation removal, to prepare the data for analysis. It then employs algorithms for Fast Global Registration and Iterative Closest Point (ICP) alignment, enabling precise change detection results.

The software identifies differences between the two epochs using the M3C2 algorithm, isolates clusters using the DBSCAN algorithm, and calculates the volumes of detected changes through alpha-shape triangulation. It is designed for automation and user-friendliness, making it accessible to non-experts in coding while efficiently processing large datasets. The libraries and software used are open-source, enhancing accessibility and collaboration.
</details>

## Installation and requirements

<details>
<summary>Click to expand</summary>

The following external open-source software is used:

| Software/Library | License | Link |
|------------------|---------|------|
| CloudCompare      | GPL     | [CloudCompare Website](https://www.danielgm.net/cc/) |

Other dependencies are managed through Python, and can be installed via `requirements.txt`.

### Installation
To set up the environment and install the required dependencies:

```bash
git clone https://github.com/xabierblanch/PyRockDiff.git
cd PyRockDiff
pip install -r requirements.txt
```
Ensure CloudCompare is downloaded and accessible from the command line.

</details>

## How It Works

This section provides a basic overview of how the code operates, what inputs are expected, and how the execution flow is controlled.

<details>
<summary>Basic Functionality</summary>

### Basic Functionality
The code is designed to automate a sequence of point cloud processing tasks, including filtering, registration, clustering, and volume computation. Each step is modular and can be executed depending on the options selected.
</details>

<details>
<summary>Input Data</summary>

### Input Data
The code expects specific input data formats:
- Point clouds (e.g., `.ply`, `.las`)
- Pre-defined parameters and paths provided through a configuration `.JSON` file.
</details>

<details>
<summary>Configuration JSON File</summary>

### Configuration JSON File
All options, paths, and parameters for the execution are controlled through a configuration file located in the `json_files` folder. This file allows the user to toggle various processing steps and fine-tune parameters without modifying the code.

| Parameter Name              | Type       | Example Value                                         |
|-----------------------------|------------|-------------------------------------------------------|
| `CloudCompare`              | String     | `"C:\\Program Files\\CloudCompare\\cloudcompare.exe"` |
| `output`                    | String     | `"C:\\Users\\XBG\\Desktop\\PyRockDiff_ICGCData"`      |

- **`CloudCompare`**: Path to the CloudCompare executable.
- **`output`**: Path where the output subsampled point clouds will be saved.

</details>

<details>
<summary>Code Structure</summary>

### Code Structure
The code follows a sequential execution pattern, but it is flexible. You can start from any step in the workflow, provided the necessary files from earlier steps are supplied as inputs. This modular approach allows skipping steps that have been completed previously or executing the entire workflow from start to finish.
</details>


## Usage Guide
<details>
<summary>1. Transform and Subsample</summary>


Transform and subsample the point clouds using CloudCompare. This step of the pipeline handles two main objectives: to convert the original point cloud from any format accepted by CloudCompare to an .xyz format and to perform spatial subsampling to reduce and equalise the number of points in the two clouds to be compared.
#### How it works:
1. **Transformation to `.xyz` format**: Converts the input point cloud to the `.xyz` format for streamlined processing in subsequent steps. During this transformation, additional attributes such as color (RGB), normals, and scalar fields are removed to reduce file size and complexity.

   
2. **Subsampling**: The point cloud is spatially subsampled to reduce point density while maintaining overall structure. The `spatial_distance` parameter (in meters), defined in the configuration `.JSON` file, controls the minimum spacing between points in the output cloud.
#### JSON file parameters:
| Parameter Name              | Type        | Example Value                                         | JSON Section     |
|-----------------------------|-------------|-------------------------------------------------------|-------------------|
| `transform_and_subsample`    | Boolean    | `true`                                                | options           |
| `spatial_distance`           | Float (cm) | `0.05`                                                | parameters        |

- **`transform_and_subsample`**: Enables or disables the transformation and subsampling step.
- **`spatial_distance`**: Specifies the minimum distance (in meters) between points for subsampling.
</details>

<details>
<summary>2. Vegetation Filter</summary>

Applies a vegetation filter using [CANUPO workflow](https://nicolas.brodu.net/common/recherche/publications/canupo.pdf) (N. Brodu and D. Lague). This consist in a simple yet efficient way to automatically classify a point cloud

#### How it works:
1. **Vegetation Filtering**: The CANUPO algorithm identifies and filters vegetation points from the input point cloud. The algorithm is integrated in the CloudCompare software and requires a `.prm` file corresponding to the classifier. A classifier for vegetation is included with the software but the user can create his own ‚Äò.prm‚Äô files using CloudCompare's CANUPO suite. The resulting filtered point cloud is saved in `.xyz` format for further analysis.

#### JSON file parameters:
| Parameter Name          | Type        | Example Value                                         | JSON Section |
|-------------------------|-------------|-------------------------------------------------------|--------------|
| `vegetation_filter`     | Boolean     | `true`                                                | options      |
| `canupo_file`           | Boolean     | `".\\bin\\canupo.prm"`                                | paths        |

- **`vegetation_filter`**: Enables or disables the vegetation filtering step.
- **`canupo_file`**: Path to the `.prm` file with the classifier

</details>

<details>
<summary>3. Cleaning Filter</summary>


Applies a statistical outlier filter to remove noise from the point cloud. This step helps enhance the quality of the data by eliminating points that are statistically different from their neighbors, ensuring more accurate analysis in subsequent steps.

#### How it works:
1. **Statistical Outlier Removal**: The outlier filter evaluates each point in the point cloud based on the distance to its neighbors. Points that have a significantly different distance compared to their local neighborhood are removed. The `nb_neighbors` parameter defines the number of neighboring points to consider, while the `std_ratio` parameter specifies the threshold for determining outliers.

#### JSON file parameters:
| Parameter Name              | Type      | Example Value                                         | JSON Section     |
|-----------------------------|-----------|-------------------------------------------------------|-------------------|
| `cleaning_filter`           | Boolean   | `true`                                                | options           |
| `nb_neighbors_f`            | Integer   | `10`                                                 | parameters        |
| `std_ratio_f`               | Float (m) | `1.5`                                                | parameters        |

- **`cleaning_filter`**: Enables or disables the application of the statistical outlier filter.
- **`nb_neighbors_f`**: Specifies the number of neighbors to consider for the statistical analysis.
- **`std_ratio_f`**: Defines the standard deviation multiplier used to identify outliers.
</details>

<details>
<summary>4. Fast Global Registration</summary>

Performs Fast Global Registration (FGR), if the <code>fast_registration</code> option is enabled. This method quickly aligns two point clouds based on their features, with the `voxel_size` parameter used to downsample the point clouds, and the registration refined through multiple iterations defined by the `ite_FGR` parameter.

#### JSON file parameters:
| Parameter Name              | Type        | Example Value                                         | JSON Section     |
|-----------------------------|-------------|-------------------------------------------------------|-------------------|
| `fast_registration`         | Boolean     | `true`                                                | options           |
| `voxel_size`               | Float       | `0.25`                                               | parameters        |
| `ite_FGR`                   | Integer     | `3`                                                  | parameters        |

- **`fast_registration`**: Enables or disables the application of the Fast Global Registration algorithm.
- **`voxel_size`**: Specifies the size of the voxel for downsampling the point clouds before registration.
- **`ite_FGR`**: Defines the number of iterations for the Fast Global Registration algorithm.

</details></details>

<details>
<summary>5. ICP Registration</summary>

Executes ICP (Iterative Closest Point) registration, if the <code>icp_registration</code> option is enabled. After initial alignment with FGR, ICP enhances the precision of the registration by iteratively minimizing the distance between corresponding points, using the `ite_ICP` parameter to specify the number of refinement iterations.

#### JSON file parameters:
| Parameter Name              | Type        | Example Value                                         | JSON Section     |
|-----------------------------|-------------|-------------------------------------------------------|-------------------|
| `icp_registration`          | Boolean     | `true`                                                | options           |
| `ite_ICP`                   | Integer     | `3`                                                  | parameters        |

- **`icp_registration`**: Enables or disables the application of the Iterative Closest Point algorithm.
- **`ite_ICP`**: Defines the number of iterations for the Iterative Closest Point algorithm.
</details>

<details>
<summary>6. ROI Focus</summary>

#### ROI Focus
<p>Performs Region of Interest (ROI) clipping on the point clouds, if the <code>roi_focus</code> option is enabled.</p>
</details>

<details>
<summary>7. M3C2 Change Detection</summary>

Calculates the differences between two point clouds using the M3C2 (Multi-Scale Model to Model Cloud Comparison) method, if the <code>m3c2_computation</code> option is enabled. This algorithm quantifies changes by analyzing the point clouds from different epochs, leveraging the specified parameters for optimal results.

#### JSON file parameters:
| Parameter Name              | Type        | Example Value                                         | JSON Section     |
|-----------------------------|-------------|-------------------------------------------------------|-------------------|
| `m3c2_dist`                 | Boolean     | `true`                                                | options           |
| `m3c2_param`                | Path        | `.\\bin\\m3c2_params.txt`                            | paths             |

- **`m3c2_dist`**: Enables or disables the application of the M3C2 algorithm to compute differences.
- **`m3c2_param`**: Path to the file containing parameters for the M3C2 calculation.</details>

<details>
<summary>8. Autoparameters</summary>

#### Auto Parameters for DBSCAN
<p>Calculates point density for DBSCAN parameters, if the <code>auto_parameters</code> option is enabled.</p>
</details>

<details>
<summary>9. Rockfall clustering</summary>

#### Rockfall Clustering (DBSCAN)
<p>Applies the DBSCAN algorithm to identify clusters in the point clouds, if the <code>rf_clustering</code> option is enabled.</p>
</details>

<details>
<summary>10. Volume Estimation</summary>

#### Volume Estimation
<p>Estimates the volume of the detected clusters, if the <code>rf_volume</code> option is enabled.</p>
</details>

## Development stages & Future Updates

<details>
<summary>Current Version TODOs</summary>

### Current Version TODOs

The following functionalities are part of the current version but require further refinement:

- [ ] Verify all paths before CloudCompare
- [ ] Enhance plots/visualizations and saving options
- [ ] Add the option to activate/deactivate additional features
- [ ] Review verbosity and debugging options
- [ ] Add a robust way to select parameters
- [ ] Create a graphical abstract

</details>

<details>
<summary>Planned Features</summary>

### Planned Features

The following features and enhancements are planned for future versions of this software:

- [ ] Implement the software for pre-failure deformation identification
- [ ] Integrate tools from [**py4dgeo**](https://github.com/3dgeo-heidelberg/py4dgeo) (MIT License)
- [ ] Provide different approaches for volume calculation
- [ ] Add AI tools for vegetation filtering
- [ ] Add AI tools to filter the wrong clusters (Blanch et al, 2020)
- [ ] Include and process RGB data (for LiDAR or SfM Point Clouds)

</details>

## Contact

<details>
<summary>Click to expand</summary>

For questions, issues, or further information regarding this software, feel free to reach out to the authors:

- **Xabier Blanch**: xabier.blanch@upc.edu

Please include a clear subject line and detailed information if reporting a bug or requesting support.

</details>

## Acknowledgments

<details>
<summary>Click to expand</summary>

We would like to thank the following individuals and institutions for their invaluable contributions and support:

- The [**RISKNAT research group**](http://www.ub.edu/risknat/) at the University of Barcelona for their past support of the doctoral research that laid the basis for this software.
- The [**Technische Universit√§t Dresden**](https://tu-dresden.de/) and [**Universitat Polit√®cnica de Catalunya**](https://www.upc.edu/en) for their assistance with the project.
- The colleagues at the [**Juniorprofessur f√ºr Geosensorsysteme**](https://tu-dresden.de/bu/umwelt/geo/ipf/geosensorsysteme) (TU Dresden) for their cooperation and support.
- The [**ICGC**](http://www.icgc.cat/) (Institut Cartogr√†fic i Geol√≤gic de Catalunya) for funding this project.
- The [**CloudCompare**](https://www.danielgm.net/cc/) and [**Open3D**](http://www.open3d.org/) open-source communities for their incredible tools and libraries.
- The [**CANUPO**](https://nicolas.brodu.net/common/recherche/publications/canupo.pdf) (Brodu & Lague) and [**M3C2**](https://www.sciencedirect.com/science/article/abs/pii/S0924271613001184) (Lague et al.) authors for developing these great algorithms.

Additionally, the methodologies used in this software are based on the work developed in the following doctoral theses in the RISKNAT research group:

| Author           | Title                                                                                                                                                                                                                                                                                                                     | Year |
|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|
| Antonio Abell√°n  | [Improvements in our understanding of rockfall phenomenon by Terrestrial Laser Scanning](https://www.researchgate.net/publication/257870480_PhD_Thesis_-_Improvements_in_our_understanding_of_rockfall_phenomenon_by_Terrestrial_Laser_Scanning_-_Emphasis_on_change_detection_and_its_application_to_spatial_prediction) | 2010 | 
| Manuel Roy√°n     | [Rockfall characterization and prediction by means of Terrestrial LiDAR](https://www.tdx.cat/handle/10803/334400#page=1)                                                                                                                                                                                                  | 2015 |
| Xabier Blanch    | [Developing Advanced Photogrammetric Methods for Automated Rockfall Monitoring](https://diposit.ub.edu/dspace/handle/2445/189157)                                                                                                                                                                                         | 2023 |


</details>

## License

<details>
<summary>Click to expand</summary>

This project is licensed under the **GNU General Public License (GPL)**. You are free to use, modify, and distribute this software under the terms of this license.

For more information, please refer to the [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html).

</details>